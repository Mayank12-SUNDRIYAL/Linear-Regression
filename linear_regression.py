# -*- coding: utf-8 -*-
"""Linear Regression

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hqCbk_VSrixZvqq4Aldmi04seBET1Yym

# Linear Regression

[Linear regression](https://en.wikipedia.org/wiki/Linear_regression) is one of the most important techniques employed in business analytics. In a linear regression problem, we are given a dataset consisting of a list of pairs of elements $(x,z(x))$, where $x$ is some controlable or observable value (e.g., number of years of study) and $z(x)$ is the target value (e.g., expected salary in the future). The goal of linear regression is to come up with a formula that will allow us to guess $z(x)$ given $x$ based on historic data.


When using **linear** regression, we assume that a certain phenomenon can be predicted by a linear expression, i.e.:

$ùë¶(ùë•)=ùëé+bùë•$,

where $x$ is the observable value and $y(x)$ is our estimatethe value we want to guess which depends linearly on $x$. In our example, if pairs $(10,50)$ and $(20,100)$ belong to our dataset, this formula would probably indicate that, if $x = 15$, the value of $y(x)$ would be 75.

Linear regression is about finding $a$ and $b$ such that the formula $y(x) = a + bx$ approximates the actual value $z(x)$ as well as possible. In general, though, the formula behind the actual value of $z(x)$ may not be linear, so we should expect our formula to make **errors**, i.e., in many cases, $y(x) \neq z(x)$.

We can cast the linear regression problem as follows: given a set $X$ of values $(x,z(x))$, our goal is to find $a$ and $b$ such that the sum of the squared errors is minimized, i.e., we want to find $a$ and $b$ that minimize the **least square error**, which is given by the following expression:

$\sum\limits_{x \in X}(y(x) - z(x))^2$

We minimize the squared errors (rather than the actual errors) because, otherwise, we would have "negative" errors canceling "positive" ones, which could lead to very bad approximations.

## Pharmacy (Extracted from the textbook)

A pharmacy has hired you to create a predictive model for determining, based on the number of hours that the pharmacy is open each week, the revenue.  Data from previous weeks is provided below:

Hours	| Revenue
---|---
	40	|5958
	44	| 6662
	48	| 6004
	48	| 6011
	60	|7250
	70	|8632
	72	|6964
	90	|11097
	100	|9107
	168	|11498

# Item 1 
Suppose that we assume a linear model, that is,

$
\textrm{Revenue} = a + b \times Hours
$

Create an optimization model to find $a$ and $b$ using least square errors as your error function, i.e., you want to find $a$ and $b$ that minimize the least square error.

## Data

For problems like this, it is extremely convenient to separate the data from the model.
"""

X = [
      40,
      44,
     48,
     48,
     60,
     70,
     72,
     90,
     100,
     168
]
Z = [
     5958,
     6662,
     6004,
     6011,
     7250,
     8632,
     6964,
     11097,
     9107,
     11498
]

"""### Solving with Python libraries

Linear regression is a very traditional problem, so there are libraries and functions in Python to solve the problem. So, before solving the problem using an optimization model, let's see how to solve it using [Scipy](https://www.scipy.org/) (in a single line!).
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from pylab import *

import matplotlib.pyplot as plt
from scipy import stats

# we are going to use scipy, so it will be better to convert our lists into numpy arrays
XX = np.array(X)
ZZ = np.array(Z)

# the values we care about are the first two returned by the function
b, a, r_value, p_value, std_err = stats.linregress(XX, ZZ)
print("a: %f    b: %f" % (a, b))

# first, let's plot the original data
plt.plot(XX, ZZ, 'o', label='original data')
# now, let's plot the line we got; note that we are using the linear regression expression as the second parameter of the plot function
plt.plot(XX, a + b*XX, 'r', label='fitted line')
plt.legend()
plt.show()

"""### Using optimization to solve the problem

Now you will see how we can use optimization to get exactly the same fitted line.
"""

# before you do anything...
# mount your drive!
# click folder on the left...
# import modules


import shutil
import sys
import os.path

if not shutil.which("pyomo"):
    !pip install -q pyomo
    assert(shutil.which("pyomo"))

if not (shutil.which("ipopt") or os.path.isfile("ipopt")):
    if "google.colab" in sys.modules:
        !wget -N -q "https://ampl.com/dl/open/ipopt/ipopt-linux64.zip"
        !unzip -o -q ipopt-linux64
        #!apt-get install -y -qq glpk-utils
    else:
        try:
            !conda install -c conda-forge ipopt
        except:
            pass

assert(shutil.which("ipopt") or os.path.isfile("ipopt"))

from pyomo.environ import *

"""**Objective Function**

$\min   \sum\limits_{i \in |X|}(y_i - z_i)^2$ `(objective function)`

**Write the Constraints**

subject to:
* $y_i = a + b \cdot x_i, \forall i \in |X|$ `(calculate estimates)`

`Domains`
* $a,b \in \mathbb{R}$ `(Parameters of linear regression)`
* $y_i \in \mathbb{R}, i \in [|X|]$ `(estimates for each data point)`

"""

# declare the model
model = ConcreteModel()

n_points = len(X)

# declare decision variables
model.a = Var(domain=Reals,initialize = 1)
model.b = Var(domain=Reals, initialize = 1)
model.y = Var([i for i in range(n_points)],domain=Reals)

# Constraints
model.constraints = ConstraintList()
# Predicted value
for i in range(n_points):
  x = X[i]
  model.constraints.add(model.y[i] == model.a + x*model.b)

# declare objective
obj_expr = 0
for i in range(n_points):
  obj_expr += (model.y[i] - Z[i])**2
model.error = Objective(
                      expr = obj_expr,
                      sense = minimize)

# show the model you've created
model.pprint()

# solve it
SolverFactory('ipopt', executable='/content/ipopt').solve(model).write()
# show the results
print("Objective value = ", model.error())
print("A = ", model.a())
print("B = ", model.b())
a = model.a()
b = model.b()

"""## Let's plot the curve (it will be similar to the one we got from Python)"""

# first, let's plot the original data
plt.plot(XX, ZZ, 'o', label='original data')
# now, let's plot the line we got; note that we are using the linear regression expression as the second parameter of the plot function
plt.plot(XX, a + b*XX, 'g', label='linear regression')
plt.legend()
plt.show()

"""# Item 2

Now we are going to see what happens if we replace linear regression for the following expression:

$
\textrm{Revenue} = a \times Hours ^ b
$

First, let's see what happens if we use the expression above and sum of squares as error measure.

**You will probably need to play with a few different initial values before you find a good solution.**
"""

# declare the model
model2 = ConcreteModel()

n_points = len(X)

# declare decision variables
model2.a = Var(domain=Reals,initialize = 1000)
model2.b = Var(domain=Reals, initialize = 1)
model2.y = Var([i for i in range(n_points)],domain=Reals)

# Constraints
model2.constraints = ConstraintList()
# Predicted value
for i in range(n_points):
  x = X[i]
  model2.constraints.add(model2.y[i] == model2.a * x**(model2.b))

# declare objective
obj_expr = 0
for i in range(n_points):
  obj_expr += (model2.y[i] - Z[i])**2
model2.error = Objective(
                      expr = obj_expr,
                      sense = minimize)

# show the model you've created
model2.pprint()

# solve it
SolverFactory('ipopt', executable='/content/ipopt').solve(model2).write()
# show the results
print("Objective value = ", model2.error())
print("A = ", model2.a())
print("B = ", model2.b())
a3 = model2.a()
b3 = model2.b()

# first, let's plot the original data
plt.plot(XX, ZZ, 'o', label='original data')
# now, let's plot the line we got; note that we are using the linear regression expression as the second parameter of the plot function
plt.plot(XX, a3*XX**b3, 'b', label='power regression')
plt.plot(XX, a + b*XX, 'g', label='linear regression')
plt.legend()
plt.show()